{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Optimal Linear Regularizers\n",
    "\n",
    "## Matthew Streeter\n",
    "\n",
    "ICML 2019\n",
    "\n",
    "https://arxiv.org/abs/1902.07234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction\n",
    "\n",
    "### Loss Function in Training\n",
    "\n",
    "empirical training loss + regularization penalty\n",
    "\n",
    "### Regularizer\n",
    "\n",
    "Explicit: L1, L2, ...\n",
    "\n",
    "Implicit: dropout, early stopping, ...\n",
    "\n",
    "### Optimal Regularizer\n",
    "\n",
    "The one that provides the tightest possible bound on **generalization gap** (i.e., difference between test and training loss).\n",
    "\n",
    "### Model of this Paper\n",
    "\n",
    "<img src=\"model.jpg\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Notation\n",
    "\n",
    "1. $\\theta$: parameters of a model\n",
    "2. $\\Theta$: a set of models\n",
    "3. $D$: an unknown distribution\n",
    "4. $z$: an instance, a <$feature, label$> pair, drawn from $D$\n",
    "5. $l$: loss function\n",
    "6. $L(\\theta) = \\mathbb{E}_{z \\sim D}[l(z,\\theta)]$: expected value of loss\n",
    "7. $\\hat{L}(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}l(z_i,\\theta)$: average training loss\n",
    "8. $R$: regularizer\n",
    "9. $f(\\theta) = \\hat{L}(\\theta) + R(\\theta)$: objective function to minimize\n",
    "10. $\\hat{\\theta} = argmin_{\\theta \\in \\Theta}\\{f(\\theta)\\}$\n",
    "11. $\\theta^{*} = argmin_{\\theta \\in \\Theta}\\{L(\\theta)\\}$\n",
    "12. $L(\\hat{\\theta}) - L(\\theta^{*})$: excess test loss\n",
    "13. choose $R$ so that the excess test loss is as small as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Regularizers and Generalization Bounds\n",
    "\n",
    "If $R(\\theta) = L(\\theta^{*}) - \\hat{L}(\\theta)$, then $min\\{f(\\theta)\\} = L(\\theta^{*})$ and $L(\\hat{\\theta}) = L(\\theta^{*})$ \n",
    "\n",
    "(according to the definition of $f$)\n",
    "\n",
    "因为 $L$ 按照定义是对**所有**数据的 loss 的期望值，因此这个定义里面就包含了**未知**的数据。如果我们找到了 optimal regularizer $R$ ，那么我们在 training 时 minimize $f$ ，就相当于同时 minimize 未知数据的 loss 了。换个说法就是，我们直接就能够在 training set 上面去 minimize test set loss 。\n",
    "\n",
    "Use a regularizer that accurately **estimates** the generalization gap.\n",
    "\n",
    "> *A good regularizer is one that provides an upper bound on the generalization gap that is tight at near-optimal points.*\n",
    "\n",
    "Training 的时候，一般都是选定一个 $R$，然后在这个固定的 $R$ 的情况下，从一堆 $\\theta$ 里面找到一个最好的 $\\hat{\\theta}$\n",
    "\n",
    "For a fixed $R$, define the *slack* of $R$ at a point $\\theta$ as\n",
    "$$\\Delta(\\theta) \\equiv f(\\theta) - L(\\theta)$$\n",
    "\n",
    "在固定的 $R$ 的情况下，会有一个最好的 $\\hat{\\theta}$，任何其他 $\\theta$ 都与 $\\hat{\\theta}$ 有一定差距\n",
    "\n",
    "For any $\\theta$, define the *suboptimality* as\n",
    "$$S(\\theta) \\equiv f(\\theta) - f(\\hat{\\theta})$$\n",
    "\n",
    "<img src=\"fig1.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面定义了 excess test loss = $L(\\hat{\\theta}) - L(\\theta^{*})$，结合 $\\Delta$ 和 $S$ 的定义，于是有\n",
    "<img src=\"SAS.jpg\">\n",
    "\n",
    "We refer to the quantity $SAS(\\theta)$ as *suboptimality-adjusted slack*.\n",
    "\n",
    "$L(\\hat{\\theta}) - L(\\theta^{*}) = max_{\\theta \\in \\Theta}\\{SAS(\\theta)\\}$\n",
    "\n",
    "That is, the excess test loss of a hypothesis $\\hat{\\theta}$ obtained by minimizing $f$ is the **worst-case suboptimality-adjusted slack**. An optimal regularizer is therefore one that minimizes this quantity. \n",
    "\n",
    "因此，我们想要找一个 $R$ ，使得 excess test loss 最小，那么我们就要找到一个 $R$ ，使得 worst-case suboptimality-adjusted slack 最小。\n",
    "\n",
    "**Proposition 1**\n",
    "$$R^{*} = argmin_{R \\in \\mathcal{R}}\\big\\{L(\\hat{\\theta}(R))\\big\\} = argmin_{R \\in \\mathcal{R}}\\big\\{max_{\\theta \\in \\Theta}\\{SAS(\\theta;R)\\}\\big\\}$$\n",
    "\n",
    "现在我们已经有了一个求最优的 $R^{*}$ 的等式了，但实际上我们还是没办法直接解出来，因为 $\\Delta$ 的定义里面是用到了 $L$ 的，但我们没办法知道每一个模型 $\\theta \\in \\Theta$ 对应的 $L$ （虽然理论上，只要给无限时间与算力，总有一天能把所有的都算出来），所以我们没有办法准确的得到 $max_{\\theta \\in \\Theta}\\{SAS(\\theta)\\}$。\n",
    "\n",
    "但是，我们是可以计算出少量模型 $\\theta \\in \\Theta_0 \\subset \\Theta$ 的 validation loss。于是我们可以计算一个 approximately optimal regularizer\n",
    "$$R^{*} \\approx argmin_{R \\in \\mathcal{R}}\\big\\{max_{\\theta \\in \\Theta_0}\\{\\hat{SAS}(\\theta;R)\\}\\big\\}$$\n",
    "where $\\hat{SAS}$ is an estimate of $SAS$ that uses validation loss\n",
    "as a proxy for test loss, and uses $\\Theta_0$ as a proxy for $\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Learning Linear Regularizers\n",
    "\n",
    "### Linear Regularizers\n",
    "\n",
    "$R(\\theta; \\lambda) = \\lambda \\cdot q(\\theta)$\n",
    "\n",
    "$q$ is a function mapping the model to a feature vector\n",
    "\n",
    "#### Example: Coin Flips\n",
    "\n",
    "$p_i \\sim Beta(\\alpha, \\beta)$\n",
    "\n",
    "$R = LogitBeta(\\theta) = -\\frac{1}{n}\\sum_{i}\\alpha log(\\theta_i) + \\beta log(1-\\theta_i)$\n",
    "\n",
    "$q(\\theta) = <-\\sum_{i}log(\\theta_i),  -\\sum_{i} log(1-\\theta_i)>$ <br/>\n",
    "$\\lambda = <\\alpha, \\beta>$\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "$V(\\theta)$: average loss on validation set\n",
    "\n",
    "Wish to find the regularizer $\\hat{R} \\equiv argmin_{R \\in \\mathcal{R}} \\{V(\\hat{\\theta}_{0}(R))\\}$\n",
    "\n",
    "<img src=\"LearnLinReg.jpg\" width=500>\n",
    "\n",
    "#### Theorm 1\n",
    "<img src=\"Theorm1.jpg\" width=500>\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "<img src=\"TuneReg.jpg\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

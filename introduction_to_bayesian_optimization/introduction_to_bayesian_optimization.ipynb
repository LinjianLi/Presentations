{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Introduction to Bayesian Optimization</h1>\n",
    "<h3 align=\"right\">Linjian Li</h3>\n",
    "<h3 align=\"right\">2018-12-22</h3>\n",
    "\n",
    "\n",
    "## Some Application Examples\n",
    "- Hyperparameter optimization: $f$ is training loss and $x$ are some hard-to-set hyperparameters\n",
    "- Recommending ads: $f$ is the expected number of clicks and $x$ are ads\n",
    "\n",
    "We want to optimize $f(x)$, but $f$ may be expensive to evaluate. So, we need to make the number of  evaluation of f as less as possible.\n",
    "\n",
    "\n",
    "## Feature\n",
    "- Global optimization\n",
    "- Black-box functions\n",
    "- Does not require derivatives\n",
    "\n",
    "\n",
    "## Basic Knowledge\n",
    "- Gaussian Process\n",
    "\n",
    "\n",
    "## Gaussian Process\n",
    "- a stochastic process\n",
    "- any finite subcollection of random variables has a multivariate Gaussian distribution\n",
    "- **mean function $m(.)$** and **covariance function (kernal) $k(., .)$**\n",
    "    - In general, $m(.)$ can be any real-valued function is acceptable, and  $k(., .)$ is positive semidefinite.\n",
    "\n",
    "\n",
    "<img src=\"GP.png\">\n",
    "\n",
    "The gray dash line is the invisible true function. <br/>\n",
    "The green solid line is the **mean** our guess of the true function. <br/>\n",
    "The green translucent green areas are the **variance** of our guess.\n",
    "\n",
    "So, which point to be sampled next?\n",
    "- Exploitation-exploration (mean-variance) trade-off\n",
    "    - Areas with high confidence of yielding better values (Exploitation)\n",
    "    - Areas with high uncertainty (Exploration)\n",
    "- Acquisition functions\n",
    "\n",
    "\n",
    "## Acquisition functions\n",
    "Acquisition functions are functions of the input space $x$ parameterized by the \n",
    "- observed data $D$\n",
    "- GP hyperparameters $\\theta$ (for now, we do not consider this term for simplicity)\n",
    "\n",
    "We use $\\alpha(x;D)$ to denote an acquisition function. They should be cheap to evaluate. (Because the objective function $f(x)$ is expensive to evaluate)\n",
    "\n",
    "Popular acquisition functions \n",
    "- probability of improvement (PI)\n",
    "    - $\\alpha_{PI}(x; D_{n}) \\gets P(f(x)>\\tau) = \\Phi(\\frac{\\mu_{n}(x)-\\tau}{\\sigma_{n}(x)})$\n",
    "- expected improvement (EI)\n",
    "    - $\\alpha_{EI}(x; D_{n}) \\gets E[I(x,f(x),\\theta)] = (\\mu_{n}(x)-\\tau)\\Phi(\\frac{\\mu_{n}(x)-\\tau}{\\sigma_{n}(x)})+\\sigma_{n}(x)\\phi(\\frac{\\mu_{n}(x)-\\tau}{\\sigma_{n}(x)})$\n",
    "- upper confidence bound (UCB) \n",
    "    - $\\alpha_{UCB}(x; D_{n}) \\gets \\mu_{n}(x)+\\beta_{n}\\sigma_{n}(x)$\n",
    "-  predictive entropy search (PES)\n",
    "\n",
    "The following image shows the $\\alpha_{EI}$ whose aim is to find the **mimimum** of $f(x)$.\n",
    "<img src=\"AC_to_find_minimum.png\">\n",
    "\n",
    "\n",
    "## Steps to Optimize Function $f(x)$\n",
    "1. Place a prior (using GP)\n",
    "2. Construct an acquisition function to determine the next query point x\n",
    "    $$ x_t = argmax_x AcquisitionFunction(x|D_{t-1}) $$\n",
    "3. Evaluate $f(x)$\n",
    "4. Form a posterior distribution (using GP)\n",
    "4. Augment data $D_{t} = \\{ D_{t-1}, (x_{t},y_{t}) \\}$\n",
    "5. go to 2\n",
    "\n",
    "\n",
    "## blah blah blah\n",
    "- Speed up the process by reducing the computation task\n",
    "- Does not expect help from the person to guess the values\n",
    "- GP is cheap to evaluate\n",
    "- ﬁnd the minimum (maximum) of difﬁcult non-convex functions with relatively **few evaluations**,<br/>\n",
    "    at the cost of performing **more computation** to determine the next point to try.\n",
    "\n",
    "\n",
    "## Example\n",
    "This example is to find the **maximum** of the function $f(x)$.\n",
    "<img src=\"iterations.png\">\n",
    "\n",
    "\n",
    "## References\n",
    "(Asterisks represents the amount of contributions. The higher, the better. Mximum is 5.)\n",
    "- http://krasserm.github.io/2018/03/21/bayesian-optimization/ $(*****)$\n",
    "- https://www.quora.com/How-does-Bayesian-optimization-work $(*****)$\n",
    "- Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & De Freitas, N. (2016). Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1), 148-175. $(****)$\n",
    "- https://www.youtube.com/watch?v=-Ekte3my510 $(***)$\n",
    "- https://towardsdatascience.com/shallow-understanding-on-bayesian-optimization-324b6c1f7083 $(*)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

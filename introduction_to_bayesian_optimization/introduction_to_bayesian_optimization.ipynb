{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\">Introduction to Bayesian Optimization</h1>\n",
    "<h3 align=\"right\">Linjian Li</h3>\n",
    "<h3 align=\"right\">2018-12-22</h3>\n",
    "\n",
    "\n",
    "## Some Application Examples $f(x)$\n",
    "- Hyperparameter optimization: $f$ is training loss and $x$ are some hard-to-set hyperparameters\n",
    "- Recommending ads: $f$ is the expected number of clicks and $x$ are ads\n",
    "\n",
    "We want to optimize $f(x)$, but $f$ may be expensive to evaluate. So, we need to make the number of  evaluation of f as less as possible. We can just take random guesses.\n",
    "<img src=\"guess optimum.png\">\n",
    "\n",
    "\n",
    "## Feature\n",
    "- Global optimization\n",
    "- For Black-box functions\n",
    "- Does not require derivatives\n",
    "\n",
    "\n",
    "\n",
    "## Gaussian Process\n",
    "- a stochastic process\n",
    "- any finite subcollection of random variables has a multivariate Gaussian distribution\n",
    "- **mean function $m(\\cdot)$** and **covariance function (kernal) $k(\\cdot, \\cdot)$**\n",
    "    - In general, $m(\\cdot)$ can be any real-valued function is acceptable. May be just some simple functions that draw a smooth curve connecting all the observed points.\n",
    "    - $k(\\cdot, \\cdot)$ is positive semidefinite.\n",
    "    <img src=\"usual covariance functions.png\">\n",
    "\n",
    "### Posterior\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\\mathbf{y} \\\\ \\mathbf{f}_*\\end{pmatrix} \\sim \\mathcal{N}\n",
    "\\left(\\begin{pmatrix}\\boldsymbol{\\mu} \\\\ \\boldsymbol{\\mu_*} \\end{pmatrix},\n",
    "\\begin{pmatrix}\\mathbf{K}_y & \\mathbf{K}_* \\\\ \\mathbf{K}_*^T & \\mathbf{K}_{**}\\end{pmatrix}\n",
    "\\right)\\tag{1}\\label{eq1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{X},\\mathbf{y}) \n",
    "&= \\int{p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{f})p(\\mathbf{f} \\lvert \\mathbf{X},\\mathbf{y})}\\ d\\mathbf{f} \\\\ \n",
    "&= \\mathcal{N}(\\mathbf{f}_* \\lvert \\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*)\n",
    "\\end{align*} \\tag{2}\\label{eq2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\mu_*} &= \\mathbf{K}_*^T \\mathbf{K}_*^{-1} \\mathbf{y} \\tag{4}\\label{eq4}\\\\\n",
    "\\boldsymbol{\\Sigma_*} &= \\mathbf{K}_{**} - \\mathbf{K}_*^T \\mathbf{K}_*^{-1} \\mathbf{K}_* \\tag{5}\\label{eq5}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"GP.png\">\n",
    "\n",
    "The gray dash line is the invisible true function. <br/>\n",
    "The green solid line is the **mean** our guess of the true function. <br/>\n",
    "The green translucent green areas are the **standard deviation** of our guess.\n",
    "\n",
    "So, which point to be sampled next?\n",
    "- Exploitation-exploration (mean-variance) trade-off\n",
    "    - Areas with high confidence of yielding better values (Exploitation)\n",
    "    - Areas with high uncertainty (Exploration)\n",
    "- Acquisition functions\n",
    "\n",
    "\n",
    "## Acquisition functions\n",
    "Acquisition functions are functions of the input space $x$ parameterized by the \n",
    "- observed data $D$\n",
    "- GP hyperparameters $\\theta$ (for now, we do not consider this term for simplicity)\n",
    "\n",
    "We use $\\alpha(x;D)$ to denote an acquisition function. They should be cheap to evaluate. (Because the objective function $f(x)$ is expensive to evaluate)\n",
    "\n",
    "Popular acquisition functions \n",
    "- probability of improvement (PI)\n",
    "    - $\\alpha_{PI}(x; D_{n}) \\gets P(f(x)>\\tau) = \\Phi(\\frac{\\mu_{n}(x)-\\tau}{\\sigma_{n}(x)})$\n",
    "- expected improvement (EI)\n",
    "    - $\\alpha_{EI}(x; D_{n}) \\gets E[I(x,f(x),\\theta)] = (\\mu_{n}(x)-\\tau)\\Phi(\\frac{\\mu_{n}(x)-\\tau}{\\sigma_{n}(x)})+\\sigma_{n}(x)\\phi(\\frac{\\mu_{n}(x)-\\tau}{\\sigma_{n}(x)})$\n",
    "- upper confidence bound (UCB) \n",
    "    - $\\alpha_{UCB}(x; D_{n}) \\gets \\mu_{n}(x)+\\beta_{n}\\sigma_{n}(x)$\n",
    "-  predictive entropy search (PES)\n",
    "\n",
    "The following image shows the $\\alpha_{EI}$ whose aim is to find the **mimimum** of $f(x)$.\n",
    "<img src=\"AC_to_find_minimum.png\">\n",
    "\n",
    "\n",
    "## Steps to Optimize Function $f(x)$\n",
    "1. Place a prior (using GP)\n",
    "2. Construct an acquisition function to determine the next query point x\n",
    "    $$ x_t = argmax_x \\alpha(x|D_{t-1}) $$\n",
    "3. Evaluate $f(x)$\n",
    "4. Form a posterior distribution (using GP)\n",
    "4. Augment data $D_{t} = \\{ D_{t-1}, (x_{t},y_{t}) \\}$\n",
    "5. go to 2\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "- ﬁnd the minimum (maximum) of difﬁcult non-convex functions with relatively **few evaluations**,<br/>\n",
    "    at the cost of performing **more computation** to determine the next point to try.\n",
    "\n",
    "\n",
    "## Example\n",
    "This example is to find the **maximum** of the function $f(x)$.\n",
    "<img src=\"iterations.png\">\n",
    "\n",
    "\n",
    "## References\n",
    "(Asterisks represents the amount of contributions. The higher, the better. Mximum is 5.)\n",
    "- https://github.com/krasserm/bayesian-machine-learning/blob/master/bayesian_optimization.ipynb $(*****)$\n",
    "- https://github.com/krasserm/bayesian-machine-learning/blob/master/gaussian_processes.ipynb $(*****)$\n",
    "- https://www.quora.com/How-does-Bayesian-optimization-work $(*****)$\n",
    "- http://katbailey.github.io/post/gaussian-processes-for-dummies/ $(*****)$\n",
    "- Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & De Freitas, N. (2016). Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1), 148-175. $(****)$\n",
    "- https://www.youtube.com/watch?v=-Ekte3my510 $(***)$\n",
    "- https://towardsdatascience.com/shallow-understanding-on-bayesian-optimization-324b6c1f7083 $(*)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2019-04-06\n",
    "\n",
    "# SMBO and Bayesian Optimization\n",
    "\n",
    "应该把SMBO和Bayesian optimization想象成一个**辅助**的工具。\n",
    "\n",
    "# 想象一个寻宝的故事\n",
    "\n",
    "现在我知道国内有许多品质不同的宝石(hyperparameter)，有烂石头、有带瑕疵的红宝石、蓝宝石、以及全世界最昂贵的一颗钻石（optimal hyperparameter）。\n",
    "\n",
    "我现在想找到一颗宝石，把它卖掉换钱（performance），钱越多越好。\n",
    "\n",
    "但我不知道它们在哪里（假设最宝贵的钻石在**海南省**）。中国有34个省级行政区，完全找遍一个地方需要花费**一天**的时间（完整地跑一次target function开销是巨大的）我只有**一周**的时间（budget是有限的）。\n",
    "\n",
    "我有几种思路去找这个宝石。\n",
    "\n",
    "### Random Search\n",
    "给34个地方编个号，掷一个34面骰子，掷出几号我就去哪找。这一周的时间我就只能随便搜寻7个地方了，如果掷7次骰子都没选中海南，那我的时间用完了也找不到钻石。我可能能找到一些烂石头，也可能找到一些普通的水晶。\n",
    "\n",
    "如果我掷骰子能掷到海南，那我就发财了。\n",
    "\n",
    "### Grid Search\n",
    "打开地图，从最左上角开始，往右往下找。这样，一周的时间我可以搜寻新疆、甘肃、内蒙古、青海、宁夏、山西、山西，7天时间用完了，我找到了一些黄金、水晶，但我没找到这个最好的钻石。\n",
    "\n",
    "如果我选择grid search的思路是从最南到最北，那我也能很快找到这个最昂贵的钻石。\n",
    "\n",
    "### SMBO / Bayesian Optimization\n",
    "拿到了一个神奇的魔镜（SMBO / Bayesian Optimization），我告诉它我之前做了什么事情，它能以80%的把握感应到这颗钻石**可能**在哪里，它能指引我下一步往哪走。\n",
    "\n",
    "最开始我什么都没做，因此用不上魔镜。我随便选一个地方，比如广东，花一天时间搜索，无结果。剩下6天。\n",
    "\n",
    "我告诉魔镜，我搜过广东了，什么都没有。魔镜花半天时间思考后 **(SMBO / Bayesian Optimization本身也有开销)** 告诉我，“钻石很有可能在上海”。剩5.5天。\n",
    "\n",
    "我花一天时间在上海搜寻，没结果，剩下4.5天。\n",
    "\n",
    "我再找魔镜，说我找过广东和上海，都没结果。魔镜又花半天时间思考，告诉我“钻石有可能在云南”。剩4天。\n",
    "\n",
    "一直重复这个过程......\n",
    "\n",
    "最后到7天时间用完的时候，我只搜索了4个地方，第5个地方还没完全搜完（7/(1+0.5)=4.67）。这期间，魔镜**非常**有可能让我去海南找找，那我就可以找到了这个最宝贵的钻石。\n",
    "\n",
    "这个魔镜**不能100%保证**我找到这个钻石，而且它思考的时候还要浪费我半天的时间，但还是愿意跟着它的指引，因为它能感应到钻石大致方位的这个超能力至少有80%的把握。如果是我自己去猜，那我猜中的可能性只有7/34，大概20%。\n",
    "\n",
    "\n",
    "# 一些不清晰的地方\n",
    "## 我的想法\n",
    "\n",
    "魔镜不能直接凭空变出钱给我，也不能凭空变出最昂贵的钻石给我。它只能告诉我钻石可能在哪里，我还是要自己去找。\n",
    "```\n",
    "SMBO不能直接给让target model得到最优performance，\n",
    "也不能直接告诉你最优的hyperparameter是什么。\n",
    "SMBO只能说：“这个hyperparameter可能挺不错的，你去试试”\n",
    "然后我们还是要跑一遍target model，\n",
    "然后再去问SMBO，“接下来我该试试哪一个呢？”\n",
    "```\n",
    "\n",
    "魔镜到底靠什么来感应钻石所在的大概方位呢？我告诉魔镜我之前探索过哪些地方，结果如何，魔镜在思考的时候就会在脑子里利用我告诉它的信息，计算出一个全中国宝石概率分布图，然后找到概率最高的那个地方，让我去那个地方找找看。\n",
    "```\n",
    "那么SMBO到底用什么来猜哪个hyperparameter可能挺好呢？\n",
    "在家焕学长发的slides的第8页的SMBO伪代码第4行右边有一个M，\n",
    "这一页最上面有个解释说M是SMBO's model。\n",
    "所以，你告诉SMBO你之前做过什么事情，结果怎么样，\n",
    "SMBO就计算一个model M来告诉你，\n",
    "接下来试一下哪个hyperparameter可能结果更好\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\">Introduction to Bayesian Optimization</h1>\n",
    "<h3 align=\"right\">Linjian Li</h3>\n",
    "<h3 align=\"right\">2018-12-22</h3>\n",
    "\n",
    "\n",
    "## Some Application Examples $f(x)$\n",
    "- Hyperparameter optimization\n",
    "- Recommending ads\n",
    "\n",
    "<img src=\"guess optimum.png\">\n",
    "\n",
    "## Feature\n",
    "- Global optimization\n",
    "- For Black-box functions\n",
    "- Does not require derivatives\n",
    "\n",
    "\n",
    "\n",
    "## Gaussian Process\n",
    "- a stochastic process\n",
    "- any finite subcollection of random variables has a multivariate Gaussian distribution\n",
    "- **mean function $m(\\cdot)$** and **covariance function (kernal) $k(\\cdot, \\cdot)$**\n",
    "    <img src=\"usual covariance functions.png\">\n",
    "\n",
    "### Posterior\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\\mathbf{y} \\\\ \\mathbf{f}_*\\end{pmatrix} \\sim \\mathcal{N}\n",
    "\\left(\\begin{pmatrix}\\boldsymbol{\\mu} \\\\ \\boldsymbol{\\mu_*} \\end{pmatrix},\n",
    "\\begin{pmatrix}\\mathbf{K}_y & \\mathbf{K}_* \\\\ \\mathbf{K}_*^T & \\mathbf{K}_{**}\\end{pmatrix}\n",
    "\\right)\\tag{1}\\label{eq1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{X},\\mathbf{y}) \n",
    "&= \\int{p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{f})p(\\mathbf{f} \\lvert \\mathbf{X},\\mathbf{y})}\\ d\\mathbf{f} \\\\ \n",
    "&= \\mathcal{N}(\\mathbf{f}_* \\lvert \\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*)\n",
    "\\end{align*} \\tag{2}\\label{eq2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\mu_*} &= \\mathbf{K}_*^T \\mathbf{K}_*^{-1} \\mathbf{y} \\tag{4}\\label{eq4}\\\\\n",
    "\\boldsymbol{\\Sigma_*} &= \\mathbf{K}_{**} - \\mathbf{K}_*^T \\mathbf{K}_*^{-1} \\mathbf{K}_* \\tag{5}\\label{eq5}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "<img src=\"GP.png\">\n",
    "\n",
    "The gray dash line is the invisible true function. <br/>\n",
    "The green solid line is the **mean** our guess of the true function. <br/>\n",
    "The green translucent green areas are the **standard deviation** of our guess.\n",
    "\n",
    "So, which point to be sampled next?\n",
    "- Exploitation-exploration (mean-variance) trade-off\n",
    "    - Exploitation\n",
    "    - Exploration\n",
    "- Acquisition functions\n",
    "\n",
    "\n",
    "## Acquisition functions $\\alpha(x;D)$\n",
    "Acquisition functions are functions of the input space $x$ parameterized by the \n",
    "- observed data $D$\n",
    "- GP hyperparameters $\\theta$\n",
    "\n",
    "Popular acquisition functions \n",
    "- probability of improvement (PI)\n",
    "    - $\\alpha_{PI}(x; D_{n}) \\gets P(f(x)>\\tau) = \\Phi(\\frac{\\mu_{n}(x)-\\tau}{\\sigma_{n}(x)})$\n",
    "- expected improvement (EI)\n",
    "    - $\\alpha_{EI}(x; D_{n}) \\gets E[I(x,f(x),\\theta)] = (\\mu_{n}(x)-\\tau)\\Phi(\\frac{\\mu_{n}(x)-\\tau}{\\sigma_{n}(x)})+\\sigma_{n}(x)\\phi(\\frac{\\mu_{n}(x)-\\tau}{\\sigma_{n}(x)})$\n",
    "- upper confidence bound (UCB) \n",
    "    - $\\alpha_{UCB}(x; D_{n}) \\gets \\mu_{n}(x)+\\beta_{n}\\sigma_{n}(x)$\n",
    "-  predictive entropy search (PES)\n",
    "\n",
    "The following image shows the $\\alpha_{EI}$ whose aim is to find the **mimimum** of $f(x)$.\n",
    "<img src=\"AC_to_find_minimum.png\">\n",
    "\n",
    "\n",
    "## Steps to Optimize Function $f(x)$\n",
    "1. Place a prior (using GP)\n",
    "2. Construct an acquisition function to determine the next query point x\n",
    "    $$ x_t = argmax_x \\alpha(x|D_{t-1}) $$\n",
    "3. Evaluate $f(x)$\n",
    "4. Form a posterior distribution (using GP)\n",
    "4. Augment data $D_{t} = \\{ D_{t-1}, (x_{t},y_{t}) \\}$\n",
    "5. go to 2\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "- ﬁnd the minimum (maximum) of difﬁcult non-convex functions with relatively **few evaluations**,<br/>\n",
    "    at the cost of performing **more computation** to determine the next point to try.\n",
    "\n",
    "\n",
    "## Example\n",
    "This example is to find the **maximum** of the function $f(x)$.\n",
    "<img src=\"iterations.png\">\n",
    "\n",
    "\n",
    "## References\n",
    "(Asterisks represents the amount of contributions. The higher, the better. Mximum is 5.)\n",
    "- https://github.com/krasserm/bayesian-machine-learning/blob/master/bayesian_optimization.ipynb $(*****)$\n",
    "- https://github.com/krasserm/bayesian-machine-learning/blob/master/gaussian_processes.ipynb $(*****)$\n",
    "- https://www.quora.com/How-does-Bayesian-optimization-work $(*****)$\n",
    "- http://katbailey.github.io/post/gaussian-processes-for-dummies/ $(*****)$\n",
    "- Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., & De Freitas, N. (2016). Taking the human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1), 148-175. $(****)$\n",
    "- https://www.youtube.com/watch?v=-Ekte3my510 $(***)$\n",
    "- https://towardsdatascience.com/shallow-understanding-on-bayesian-optimization-324b6c1f7083 $(*)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
